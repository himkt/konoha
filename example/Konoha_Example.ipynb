{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Konoha Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfRY3Eil17m3Ji26xdfkPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himkt/konoha/blob/master/example/Konoha_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG0CWJfYgdQQ",
        "colab_type": "text"
      },
      "source": [
        "# 環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBxBPzHtYYXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1b00e8f0-3756-4a98-ccc3-65f74f2bb02c"
      },
      "source": [
        "!sudo apt update -qy && apt install -qy mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install -q konoha[all]\n",
        "!mkdir data && wget -q https://github.com/himkt/konoha/raw/master/data/model.spm -O data/model.spm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "60 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libmecab-dev is already the newest version (0.996-5).\n",
            "mecab is already the newest version (0.996-5).\n",
            "mecab-ipadic-utf8 is already the newest version (2.7.0-20070801+main-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji945j5ngfYf",
        "colab_type": "text"
      },
      "source": [
        "# モジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db4FhCLbZusn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konoha import SentenceTokenizer\n",
        "from konoha import WordTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOLJXDRgh1B",
        "colab_type": "text"
      },
      "source": [
        "# トークナイザの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hss2yijZ3Hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0d7e1b5-a356-4523-dc77-722d1c9e80f5"
      },
      "source": [
        "sentence_tokenizer = SentenceTokenizer()\n",
        "tokenizers = [\"MeCab\", \"Janome\", \"nagisa\", \"Character\"]\n",
        "tokenizers_support_postag = [\"MeCab\", \"Janome\", \"nagisa\"]\n",
        "\n",
        "word_tokenizers = []\n",
        "for word_tokenizer_name in tokenizers:\n",
        "    tokenizer = WordTokenizer(word_tokenizer_name, with_postag=True)\n",
        "    word_tokenizers.append(tokenizer)\n",
        "\n",
        "tokenizer = WordTokenizer(\"Sentencepiece\", model_path=\"./data/model.spm\")\n",
        "word_tokenizers.append(tokenizer)\n",
        "\n",
        "# FIXME: `ContextualVersionConflict` happens on Google Colaboratory.\n",
        "#        If you want to try `Sudachi`, please restart this notebook\n",
        "#        after running the entire cells once.\n",
        "#\n",
        "# ref. https://github.com/nipy/niwidgets/issues/57\n",
        "#\n",
        "# tokenizer = WordTokenizer(\"Sudachi\", mode=\"A\", with_postag=True)\n",
        "# word_tokenizers.append(tokenizer)\n",
        "\n",
        "print(\"Finish creating word tokenizers\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish creating word tokenizers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggH8kKFKgkIg",
        "colab_type": "text"
      },
      "source": [
        "# データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arisOWTHfmwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7e3b62f-7b90-45bd-e92e-13851cd42d66"
      },
      "source": [
        "document = \"東京特許許可局（とうきょうとっきょきょかきょく） 日本語の早口言葉。\"\n",
        "document += \"なお実際に特許に関する行政を行うのは特許庁であり、過去にこのような役所が存在したことは一度も無い。\"\n",
        "\n",
        "print(\"Document: {}\".format(document))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document: 東京特許許可局（とうきょうとっきょきょかきょく） 日本語の早口言葉。なお実際に特許に関する行政を行うのは特許庁であり、過去にこのような役所が存在したことは一度も無い。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5rOEqn4f8zm",
        "colab_type": "text"
      },
      "source": [
        "## 文分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zJh4_SsfrH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87d67e9e-433e-4866-b1ec-9fafddf2f330"
      },
      "source": [
        "for index, sentence in enumerate(sentence_tokenizer.tokenize(document)):\n",
        "    print(f\"sentence #{index}: {sentence}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence #0: 東京特許許可局（とうきょうとっきょきょかきょく） 日本語の早口言葉。\n",
            "sentence #1: なお実際に特許に関する行政を行うのは特許庁であり、過去にこのような役所が存在したことは一度も無い。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41yNPUXygA15",
        "colab_type": "text"
      },
      "source": [
        "# 単語分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdxqmrp_fa3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9a2f8a32-343f-48c9-a45b-2e8d21641b59"
      },
      "source": [
        "sentence = sentence_tokenizer.tokenize(document)[0]\n",
        "\n",
        "for word_tokenizer in word_tokenizers:\n",
        "    print(\"Tokenizer: {}\".format(word_tokenizer.name))\n",
        "    result = word_tokenizer.tokenize(sentence)\n",
        "    result = [str(r) for r in result]\n",
        "    print(\" \".join(result))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer: mecab\n",
            "東京 (名詞) 特許 (名詞) 許可 (名詞) 局 (名詞) （ (記号) とうき (名詞) ょうとっきょきょかきょく (名詞) ） (記号) 日本語 (名詞) の (助詞) 早口 (名詞) 言葉 (名詞) 。 (記号)\n",
            "Tokenizer: janome\n",
            "東京 (名詞) 特許 (名詞) 許可 (名詞) 局 (名詞) （ (記号) とうき (名詞) ょうとっきょきょかきょく (名詞) ） (記号)   (記号) 日本語 (名詞) の (助詞) 早口 (名詞) 言葉 (名詞) 。 (記号)\n",
            "Tokenizer: nagisa\n",
            "東京 (名詞) 特許 (名詞) 許可 (名詞) 局 (名詞) ( (補助記号) とうきょう (名詞) とっきょ (名詞) きょ (記号) かきょく (名詞) ) (補助記号) 　 (空白) 日本 (名詞) 語 (名詞) の (助詞) 早口 (名詞) 言葉 (名詞) 。 (補助記号)\n",
            "Tokenizer: character\n",
            "東 京 特 許 許 可 局 （ と う き ょ う と っ き ょ き ょ か き ょ く ）   日 本 語 の 早 口 言 葉 。\n",
            "Tokenizer: sentencepiece\n",
            "▁東京 特許 許可 局 ( とうきょう と っ きょ きょ か きょく ) ▁ 日本語の 早 口 言葉 。\n",
            "Tokenizer: sudachi (a)\n",
            "東京 (名詞) 特許 (名詞) 許可 (名詞) 局 (名詞) （ (補助記号) とうきょう (名詞) とっ (動詞) き (助動詞) ょ (助詞) き (助動詞) ょ (助詞) か (助詞) きょく (名詞) ） (補助記号)   (空白) 日本 (名詞) 語 (名詞) の (助詞) 早 (形容詞) 口言葉 (名詞) 。 (補助記号)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7jP9mIEgQiY",
        "colab_type": "text"
      },
      "source": [
        "# ホワイトスペースで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycR7DX8vgOV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e17720e-205a-4a62-ceea-6dca632dd4a8"
      },
      "source": [
        "word_tokenizer = WordTokenizer(\"whitespace\")\n",
        "sentence = \"私 は 猫 だ ニャン\"\n",
        "print(word_tokenizer.tokenize(sentence))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[私, は, 猫, だ, ニャン]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}