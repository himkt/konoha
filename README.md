# ğŸŒ¿ Konoha: Simple wrapper of Japanese Tokenizers

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/himkt/konoha/blob/main/example/Konoha_Example.ipynb)
<p align="center"><img src="https://user-images.githubusercontent.com/5164000/120913279-e7d62380-c6d0-11eb-8d17-6571277cdf27.gif" width="95%"></p>

[![GitHub stars](https://img.shields.io/github/stars/himkt/konoha?style=social)](https://github.com/himkt/konoha/stargazers)

[![Downloads](https://pepy.tech/badge/konoha)](https://pepy.tech/project/konoha)
[![Downloads](https://pepy.tech/badge/konoha/month)](https://pepy.tech/project/konoha/month)
[![Downloads](https://pepy.tech/badge/konoha/week)](https://pepy.tech/project/konoha/week)

[![Build Status](https://github.com/himkt/konoha/workflows/Python%20package/badge.svg?style=flat-square)](https://github.com/himkt/konoha/actions)
[![Documentation Status](https://readthedocs.org/projects/konoha/badge/?version=latest)](https://konoha.readthedocs.io/en/latest/?badge=latest)
![Python](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue?logo=python)
[![PyPI](https://img.shields.io/pypi/v/konoha.svg)](https://pypi.python.org/pypi/konoha)
[![GitHub Issues](https://img.shields.io/github/issues/himkt/konoha.svg?cacheSeconds=60&color=yellow)](https://github.com/himkt/konoha/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/himkt/konoha.svg?cacheSeconds=60&color=yellow)](https://github.com/himkt/konoha/issues)

`Konoha` is a Python library for providing easy-to-use integrated interface of various Japanese tokenizers,
which enables you to switch a tokenizer and boost your pre-processing.

## Supported tokenizers

<a href="https://github.com/buruzaemon/natto-py"><img src="https://img.shields.io/badge/MeCab-natto--py-ff69b4"></a>
<a href="https://github.com/chezou/Mykytea-python"><img src="https://img.shields.io/badge/KyTea-Mykytea--python-ff69b4"></a>
<a href="https://github.com/mocobeta/janome"><img src="https://img.shields.io/badge/Janome-janome-ff69b4"></a>
<a href="https://github.com/WorksApplications/SudachiPy"><img src="https://img.shields.io/badge/Sudachi-sudachipy-ff69b4"></a>
<a href="https://github.com/google/sentencepiece"><img src="https://img.shields.io/badge/Sentencepiece-sentencepiece-ff69b4"></a>
<a href="https://github.com/taishi-i/nagisa"><img src="https://img.shields.io/badge/nagisa-nagisa-ff69b4"></a>

Also, `konoha` provides rule-based tokenizers (whitespace, character) and a rule-based sentence splitter.


## Quick Start with Docker

Simply run followings on your computer:

```bash
docker run --rm -p 8000:8000 -t himkt/konoha  # from DockerHub
```

Or you can build image on your machine:

```bash
git clone https://github.com/himkt/konoha  # download konoha
cd konoha && docker-compose up --build  # build and launch container
```

Tokenization is done by posting a json object to `localhost:8000/api/v1/tokenize`.
You can also batch tokenize by passing `texts: ["ï¼‘ã¤ç›®ã®å…¥åŠ›", "ï¼’ã¤ç›®ã®å…¥åŠ›"]` to `localhost:8000/api/v1/batch_tokenize`.

(API documentation is available on `localhost:8000/redoc`, you can check it using your web browser)

Send a request using `curl` on your terminal.
Note that a path to an endpoint is changed in v4.6.4.
Please check our release note (https://github.com/himkt/konoha/releases/tag/v4.6.4).

```json
$ curl localhost:8000/api/v1/tokenize -X POST -H "Content-Type: application/json" \
    -d '{"tokenizer": "mecab", "text": "ã“ã‚Œã¯ãƒšãƒ³ã§ã™"}'

{
  "tokens": [
    [
      {
        "surface": "ã“ã‚Œ",
        "part_of_speech": "åè©"
      },
      {
        "surface": "ã¯",
        "part_of_speech": "åŠ©è©"
      },
      {
        "surface": "ãƒšãƒ³",
        "part_of_speech": "åè©"
      },
      {
        "surface": "ã§ã™",
        "part_of_speech": "åŠ©å‹•è©"
      }
    ]
  ]
}
```


## Installation


I recommend you to install konoha by `pip install 'konoha[all]'`.

- Install konoha with a specific tokenizer: `pip install 'konoha[(tokenizer_name)]`.
- Install konoha with a specific tokenizer and remote file support: `pip install 'konoha[(tokenizer_name),remote]'`

If you want to install konoha with a tokenizer, please install konoha with a specific tokenizer
(e.g. `konoha[mecab]`, `konoha[sudachi]`, ...etc) or install tokenizers individually.


## Example

### Word level tokenization

```python
from konoha import WordTokenizer

sentence = 'è‡ªç„¶è¨€èªå‡¦ç†ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™'

tokenizer = WordTokenizer('MeCab')
print(tokenizer.tokenize(sentence))
# => [è‡ªç„¶, è¨€èª, å‡¦ç†, ã‚’, å‹‰å¼·, ã—, ã¦, ã„, ã¾ã™]

tokenizer = WordTokenizer('Sentencepiece', model_path="data/model.spm")
print(tokenizer.tokenize(sentence))
# => [â–, è‡ªç„¶, è¨€èª, å‡¦ç†, ã‚’, å‹‰å¼·, ã—, ã¦ã„ã¾ã™]
```

For more detail, please see the `example/` directory.

### Remote files

Konoha supports dictionary and model on cloud storage (currently supports Amazon S3).
It requires installing konoha with the `remote` option, see [Installation](#installation).

```python
# download user dictionary from S3
word_tokenizer = WordTokenizer("mecab", user_dictionary_path="s3://abc/xxx.dic")
print(word_tokenizer.tokenize(sentence))

# download system dictionary from S3
word_tokenizer = WordTokenizer("mecab", system_dictionary_path="s3://abc/yyy")
print(word_tokenizer.tokenize(sentence))

# download model file from S3
word_tokenizer = WordTokenizer("sentencepiece", model_path="s3://abc/zzz.model")
print(word_tokenizer.tokenize(sentence))
```

### Sentence level tokenization

```python
from konoha import SentenceTokenizer

sentence = "ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚"

tokenizer = SentenceTokenizer()
print(tokenizer.tokenize(sentence))
# => ['ç§ã¯çŒ«ã ã€‚', 'åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚', 'ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']
```

You can change symbols for a sentence splitter and bracket expression.

1. sentence splitter

```python
sentence = "ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ï¼ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚"

tokenizer = SentenceTokenizer(period="ï¼")
print(tokenizer.tokenize(sentence))
# => ['ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ï¼', 'ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']
```

2. bracket expression

```python
sentence = "ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚ã ãŒï¼Œã€ã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚"

tokenizer = SentenceTokenizer(
    patterns=SentenceTokenizer.PATTERNS + [re.compile(r"ã€.*?ã€")],
)
print(tokenizer.tokenize(sentence))
# => ['ç§ã¯çŒ«ã ã€‚', 'åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚', 'ã ãŒï¼Œã€ã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']
```


## Test

```
python -m pytest
```

## Article

- [ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã„ã„æ„Ÿã˜ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª konoha ã‚’ä½œã£ãŸ](https://qiita.com/klis/items/bb9ffa4d9c886af0f531)
- [æ—¥æœ¬èªè§£æãƒ„ãƒ¼ãƒ« Konoha ã« AllenNLP é€£æºæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ãŸ](https://qiita.com/klis/items/f1d29cb431d1bf879898)

## Acknowledgement

Sentencepiece model used in test is provided by @yoheikikuta. Thanks!
